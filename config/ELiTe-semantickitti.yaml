# Config format schema number
format_version: 1

###################
## Model options
model_params:
  model_architecture: "arch_elite"

  input_dims: 4
  spatial_shape:
    - 1000
    - 1000
    - 60
  scale_list:
    - 2
    - 4
    - 8
    - 16

  hiden_size: 64
  num_classes: 20
  backbone_2d: vit_b
  pretrained2d: True
  pretrained3d: False
  image_encoder:
    img_size: 1024
    embed_dim: 768
    hiden_size: 96
    depth: 12
    num_heads: 12
    global_attn_indexes: [ 2, 5, 8, 11 ]
    trainable_params:
      - lora_
      - output_upscaling
    pool_1d: upscale
    postprocess: interpolate
    peft: adalora
    cut: True


###################
## Dataset options
dataset_params:
  training_size: 19132
  dataset_type: "point_image_dataset_semkitti_sam"
  pc_dataset_type: "SemanticKITTI"
  collate_type: "collate_fn_default"
  ignore_label: 0
  label_mapping: "./config/label_mapping/semantic-kitti.yaml"

  # 2D augmentation
  bottom_crop:
    - 1024
    - 320
  color_jitter:
    - 0.4
    - 0.4
    - 0.4
  flip2d: 0.5
  image_normalizer:
    - [ 123.675, 116.28, 103.53 ]
    - [ 58.395, 57.12, 57.375 ]
  max_volume_space:
    - 50
    - 50
    - 2
  min_volume_space:
    - -50
    - -50
    - -4
  seg_labelweights:
    - 0
    - 55437630
    - 320797
    - 541736
    - 2578735
    - 3274484
    - 552662
    - 184064
    - 78858
    - 240942562
    - 17294618
    - 170599734
    - 6369672
    - 230413074
    - 101130274
    - 476491114
    - 9833174
    - 129609852
    - 4506626
    - 1168181

  train_data_loader:
    data_path: "../dataset/sequences/"
    batch_size: 4
    shuffle: True
    num_workers: 8
    rotate_aug: True
    flip_aug: True
    scale_aug: True
    transform_aug: True
    dropout_aug: True

  val_data_loader:
    data_path: "../dataset/sequences/"
    shuffle: False
    num_workers: 8

    # normal test
    batch_size: 8
    rotate_aug: False
    flip_aug: False
    scale_aug: False
    transform_aug: False
    dropout_aug: False


###################
## Train params
train_params:
  max_num_epochs: 64
  learning_rate: 0.24
  optimizer: SGD  # [SGD, Adam]
  lr_scheduler: CosineAnnealingWarmRestarts  # [StepLR, ReduceLROnPlateau, CosineAnnealingLR, CosineAnnealingWarmRestarts]
  momentum: 0.9
  nesterov: True
  weight_decay: 1.0e-4

  lambda_seg2d: 1
  lambda_xm: 0.05
